# -*- coding: utf-8 -*-
"""Sentiment_Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1l1Q0tBc30kGU24xVlSupqLtH98Y6sXO5
"""

#importing the necessary libraries

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from keras.models import Sequential
from keras.layers import Dense
import nltk
import re
from bs4 import BeautifulSoup
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.preprocessing import LabelBinarizer

nltk.download('stopwords')
nltk.download('wordnet')

# importing the training data

df=pd.read_csv('IMDB Dataset.csv')

print(df.shape)
df.head(10)

#summary of the training data

df.describe()

#Checking the sentiment count

print("postive ",np.sum(df["sentiment"]=="positive"))
print("negative ",np.sum(df["sentiment"]=="negative"))

#The dataset is balanced

#Clearing the noise in the data - such as html blocks and special characters

def clear_noise(text):
  
  # removing html scripts using Beautiful Soup
  text1=BeautifulSoup(text)
  text1=text1.get_text()

  #removing all other character than alphabets and numbers using regular expression
  text2=re.sub('[^a-zA-Z0-9\s]', '', text1)

  return text2

df["review"]=df["review"].apply(clear_noise)
df.head()

# All html and other special characters got removed

# Now preprocessing the data according to the bag of word model

# 1. Tokenisation (Breaking down the document into words)

def tokenize(text):
  return text.split()

# 2. Stopword Removal (Removal of words which are not meaningful for the model)

sw=set(nltk.corpus.stopwords.words('english'))

def stopword_removal(text):
  useful_words=[w for w in text if w not in sw]
  return useful_words

# 3. Lemmatization (Changing all forms of a verb to root form like plays ,played etc. to play)

wn=nltk.stem.WordNetLemmatizer()

def lemmatize(text):
  words=[wn.lemmatize(w) for w in text]
  return words

# 4. Building a vocabulary (Each sentence will have a feature vector)

def myTokenizer(text):
  text1=tokenize(text.lower())
  text1=stopword_removal(text1)
  text1=lemmatize(text1)
  return text1

cv=CountVectorizer(tokenizer=myTokenizer)

print(sw)

#Splitting the training data - (Training and Testing data)

train_x=df.review[:40000]
train_y=df.sentiment[:40000]

test_x=df.review[40000:]
test_y=df.sentiment[40000:]

print(train_x.shape,type(train_x))
print(test_x.shape,type(test_x))

# Applying all the above operations on the training data

vectorised_data=cv.fit_transform(train_x)

print(vectorised_data.shape)

# Converting the test and train data in vectorised form

cv_train=cv.transform(train_x)
cv_test=cv.transform(test_x)

print(cv_train.shape)

# Labelling the sentiment data

lb=LabelBinarizer()
sentiment_data=lb.fit_transform(df['sentiment'])
print(sentiment_data.shape)

train_sentiments=sentiment_data[:40000]
test_sentiments=sentiment_data[40000:]

# Building the MLP (Multi layer perceptron) Model

model=Sequential()
model.add(Dense(16,activation="relu",input_shape=(181861,)))
model.add(Dense(16,activation="relu"))
model.add(Dense(1,activation="sigmoid"))

model.compile(optimizer='sgd',loss='binary_crossentropy',metrics=['accuracy'])

model.summary()

hist=model.fit(cv_train,train_sentiments,epochs=60,batch_size=512)

h=hist.history
plt.plot(h['loss'],label="training_loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.show()

plt.plot(h['accuracy'],label="training_accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.show()

model.evaluate(cv_test,test_sentiments)

# Got around 89 percent accuracy.

